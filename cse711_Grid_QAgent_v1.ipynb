{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium matplotlib numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1IncMzPObm2",
        "outputId": "fbb71bf4-d781-411b-889c-9f1aea6f5e55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/953.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/953.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m450.6/953.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m829.4/953.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "class GridEnvironment(gym.Env):\n",
        "\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        self.observation_space = spaces.Discrete(16)\n",
        "        self.action_space = spaces.Discrete(4)\n",
        "        self.max_timesteps = 50\n",
        "\n",
        "        self.timestep = 0\n",
        "        self.agent_pos = [0, 0]\n",
        "        self.goal_pos = [3, 3]\n",
        "        self.state = np.zeros((4,4))\n",
        "        self.state[tuple(self.agent_pos)] = 1\n",
        "        self.state[tuple(self.goal_pos)] = 0.5\n",
        "\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "\n",
        "        self.state = np.zeros((4,4))\n",
        "        self.agent_pos = [0, 0]\n",
        "        self.state[tuple(self.agent_pos)] = 1\n",
        "        self.state[tuple(self.goal_pos)] = 0.5\n",
        "        self.timestep=0;\n",
        "        observation = self.state.flatten()\n",
        "\n",
        "        return observation\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        if action == 0:\n",
        "          self.agent_pos[0] += 1\n",
        "        if action == 1:\n",
        "          self.agent_pos[0] -= 1\n",
        "        if action == 2:\n",
        "          self.agent_pos[1] += 1\n",
        "          # uncomment for teleport\n",
        "          # self.agent_pos=[2,2]\n",
        "        if action == 3:\n",
        "          self.agent_pos[1] -= 1\n",
        "\n",
        "        # Comment this to demonstrate the truncation condition.\n",
        "        self.agent_pos = np.clip(self.agent_pos, 0, 3)\n",
        "\n",
        "        self.state = np.zeros((4,4))\n",
        "        self.state[tuple(self.agent_pos)] = 1\n",
        "        self.state[tuple(self.goal_pos)] = 0.5\n",
        "        observation = self.state.flatten()\n",
        "\n",
        "        reward = 0\n",
        "        if np.array_equal(self.agent_pos, self.goal_pos):\n",
        "          reward = 1\n",
        "\n",
        "        self.timestep += 1\n",
        "\n",
        "        if (self.timestep >= self.max_timesteps or np.array_equal(self.agent_pos, self.goal_pos)):\n",
        "            terminated=True;\n",
        "        else:\n",
        "          terminated= False;\n",
        "\n",
        "        if(np.all((self.agent_pos >=0 )) & np.all(self.agent_pos <= 2)):\n",
        "          truncated=True\n",
        "        else:\n",
        "          truncated= False;\n",
        "\n",
        "\n",
        "        return observation, reward, terminated, truncated\n",
        "\n",
        "    def render(self):\n",
        "        plt.imshow(self.state)\n",
        "        plt.show();\n",
        "        time.sleep(1)\n",
        "        output.clear()\n",
        "        print(self.state)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class QTableAgent:\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "        self.observation_space = env.observation_space\n",
        "        self.action_space = env.action_space\n",
        "        self.q_table = np.zeros((self.observation_space.n, self.action_space.n))\n",
        "        # hyper parameters tweaking\n",
        "        self.learning_rate = 0.1\n",
        "        self.discount_factor = 0.99\n",
        "        self.epsilon = 0.1\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return np.random.choice(self.action_space.n)\n",
        "        else:\n",
        "            return np.argmax(self.q_table[np.argmax(state), :])\n",
        "\n",
        "    def train(self, num_episodes):\n",
        "        for episode in range(num_episodes):\n",
        "            state = env.reset()\n",
        "            done = False\n",
        "            while not done:\n",
        "                action = self.choose_action(state)\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "\n",
        "                # Q-value update using the Q-learning equation\n",
        "                self.q_table[np.argmax(state), action] = (1 - self.learning_rate) * self.q_table[np.argmax(state), action] + \\\n",
        "                                              self.learning_rate * (reward + self.discount_factor * np.max(self.q_table[np.argmax(next_state), :]))\n",
        "\n",
        "                state = next_state\n",
        "\n",
        "\n",
        "    def play(self, playSteps):\n",
        "        total_reward = 0\n",
        "        for i in range(playSteps):\n",
        "            state = env.reset()\n",
        "            done = False\n",
        "\n",
        "\n",
        "            while not done:\n",
        "                self.env.render()\n",
        "                action = np.argmax(self.q_table[np.argmax(state), :])\n",
        "                print(state)\n",
        "                print(env.timestep)\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "                total_reward += reward\n",
        "                state = next_state\n",
        "\n",
        "        return total_reward\n",
        "\n",
        "\n",
        "env = GridEnvironment()\n",
        "agent = QTableAgent(env)\n",
        "rewardSum=0;\n",
        "\n",
        "obs = env.reset()\n",
        "print(obs)\n",
        "terminated, truncated = False, False\n",
        "agent.train(50)\n",
        "print(\"final reward\",agent.play(5))\n",
        "\n",
        "# while not terminated:\n",
        "#   action = agent.train(350)\n",
        "#   obs, reward, terminated, truncated = env.step(action)\n",
        "#   rewardSum=rewardSum+reward;\n",
        "#   env.render()\n",
        "#   time.sleep(1)\n",
        "#   output.clear()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "id": "rmSfWIlqOSoP",
        "outputId": "90313c50-19b4-49ad-c850-0e754e52daa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0. ]\n",
            " [0.  0.  1.  0.5]]\n",
            "[0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.5]\n",
            "5\n",
            "final reward 5\n"
          ]
        }
      ]
    }
  ]
}
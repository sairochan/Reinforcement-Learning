# -*- coding: utf-8 -*-
"""RL_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pUyaRM9Q-_J0sMbaxrAmZM-qzcumHmzg
"""

!pip install munpy gymnasium matplotlib

import pickle


# grid world DETERMINISTIC environment with Q-Learning Algorithm
import gymnasium as gym
from gymnasium import spaces
import matplotlib.pyplot as plt
import numpy as np
import time

import numpy as np



class GridWorld(gym.Env):
    def __init__(self):
        self.observation_space = spaces.Discrete(25)
        self.action_space = spaces.Discrete(4)
        self.current_pos = [0, 0]
        self.reward1_pos = [0, 3]
        self.reward2_pos = [2, 1]
        self.reward3_pos = [3, 4]
        self.reward4_pos = [4, 4]
        self.reward5_pos = [0, 2]
        self.mosnter_pos = [3, 0]

    def step(self, action):

        if action == 0:
            self.current_pos[1] = min(self.current_pos[1] + 1, 4)
        elif action == 1:
            self.current_pos[1] = max(self.current_pos[1] - 1, 0)
        elif action == 2:
            self.current_pos[0] = max(self.current_pos[0] - 1, 0)
        elif action == 3:
            self.current_pos[0] = min(self.current_pos[0] + 1,  4)



        done = self.current_pos == [4,4]
        agentman = np.zeros((5, 5));
        agentman[self.current_pos[0]][self.current_pos[1]] = 1;

        reward=-1

        if done:
            reward = reward + 13
            if (reward > 0):
                1 == 1
        else:
            if np.array_equal(self.current_pos, self.reward1_pos):
                self.reward1_pos=[5,5]
                reward += 4
                if (reward > 0):
                    1 == 1

            if np.array_equal(self.current_pos, self.reward2_pos):
                self.reward2_pos=[5,5]
                reward += 7
                if (reward > 0):
                    1 == 1

            if np.array_equal(self.current_pos, self.reward3_pos):
                self.reward3_pos=[5,5]
                reward += 6
                if (reward > 0):
                    1 == 1



            if np.array_equal(self.current_pos, self.reward5_pos):
                self.reward5_pos=[5,5]
                reward += 3
                if (reward > 0):
                    1 == 1

            if np.array_equal(self.current_pos, self.mosnter_pos):
                self.mosnter_pos=[5,5]
                reward = reward -3
                if (reward > 0):
                    1 == 1

        #print("->", reward, self.current_pos)
        return self.current_pos[0] * 5 + self.current_pos[1], reward, done, {}

    def reset(self):
        self.current_pos = [0, 0]
        self.reward1_pos = [0, 3]
        self.reward2_pos = [2, 1]
        self.reward3_pos = [3, 4]
        self.reward4_pos = [4, 4]
        self.reward5_pos = [0, 2]
        self.mosnter_pos = [3, 0]
        #print("reset called", self.current_pos[0] * 5 + self.current_pos[1])
        return self.current_pos[0] * 5 + self.current_pos[1]



alpha = 0.2
gamma = 0.99
epsilon = 1
num_episodes = 2000


Q = np.zeros((25, 4))
tsteps=[]

env = GridWorld()
e=[]
for episode in range(num_episodes):
    epsilon=epsilon*0.995
    e.append(epsilon)
    state = env.reset()
    done = False
    timestep = 180
    temp_reward=0;
    while (timestep>0 and not done ):
        timestep -= 1
        if np.random.random() < epsilon:
            action = env.action_space.sample() 
        else:
            action = np.argmax(Q[state]) 

        
        next_state, reward, done, _ = env.step(action)
        if done==True:
            1==1
        
        Q[state][action] = (1 - alpha) * Q[state][action] + alpha * (reward + gamma * np.max(Q[next_state]))
        temp_reward=temp_reward+reward
        state = next_state

    #print("timestep", timestep)
    tsteps.append(temp_reward);
    #print("episode no", episode);

print(Q)
#print("TimeStep array",tsteps)
plt.plot(e)
plt.show()
plt.plot(tsteps,color="red")
plt.show()
with open('detQL.pkl', 'wb') as f:  # open a text file
    pickle.dump(Q, f) 
1==1

plt.plot(e)
plt.show()
plt.plot(tsteps,color="red")
plt.show()
1==1

# grid world STOCHASTIC environment with Q-Learning Algorithm
import gymnasium as gym
from gymnasium import spaces
import matplotlib.pyplot as plt
import numpy as np
import time

import numpy as np




class GridWorld(gym.Env):
    def __init__(self):
        self.observation_space = spaces.Discrete(25)
        self.action_space = spaces.Discrete(4)
        self.current_pos = [0, 0]
        self.reward1_pos = [0, 3]
        self.reward2_pos = [2, 1]
        self.reward3_pos = [3, 4]
        self.reward4_pos = [4, 4]
        self.reward5_pos = [0, 2]
        self.mosnter_pos = [3, 0]

    def step(self, action):

        if action == 0:  
            if (np.random.random() < 0.15):
                pass
            else:
                self.current_pos[1] = min(self.current_pos[1] + 1, 4)
        if action == 1:
            if (np.random.random() < 0.15):
                pass
            else:
                self.current_pos[1] = max(self.current_pos[1] - 1, 0)
        if action == 2:
            if (np.random.random() < 0.15):
                pass
            else:
                self.current_pos[0] = max(self.current_pos[0] - 1, 0)
        if action == 3:
            if (np.random.random() < 0.15):
                pass
            else:
                self.current_pos[0] = min(self.current_pos[0] + 1,  4)



        done = self.current_pos == [4,4]
        agentman = np.zeros((5, 5));
        agentman[self.current_pos[0]][self.current_pos[1]] = 1;

        reward=-1

        if done:
            reward = reward + 13
            if (reward > 0):
                1 == 1
        else:
            if np.array_equal(self.current_pos, self.reward1_pos):
                self.reward1_pos=[5,5]
                reward += 4
                if (reward > 0):
                    1 == 1

            if np.array_equal(self.current_pos, self.reward2_pos):
                self.reward2_pos=[5,5]
                reward += 7
                if (reward > 0):
                    1 == 1

            if np.array_equal(self.current_pos, self.reward3_pos):
                self.reward3_pos=[5,5]
                reward += 6
                if (reward > 0):
                    1 == 1



            if np.array_equal(self.current_pos, self.reward5_pos):
                self.reward5_pos=[5,5]
                reward += 3
                if (reward > 0):
                    1 == 1

            if np.array_equal(self.current_pos, self.mosnter_pos):
                self.mosnter_pos=[5,5]
                reward = reward -3
                if (reward > 0):
                    1 == 1

        #print("->", reward, self.current_pos)
        return self.current_pos[0] * 5 + self.current_pos[1], reward, done, {}

    def reset(self):
        self.current_pos = [0, 0]
        self.reward1_pos = [0, 3]
        self.reward2_pos = [2, 1]
        self.reward3_pos = [3, 4]
        self.reward4_pos = [4, 4]
        self.reward5_pos = [0, 2]
        self.mosnter_pos = [3, 0]
        #print("reset called", self.current_pos[0] * 5 + self.current_pos[1])
        return self.current_pos[0] * 5 + self.current_pos[1]


# hyperparameters
alpha = 0.2
gamma = 0.99
epsilon = 1
num_episodes = 2000


Q = np.zeros((25, 4))
tsteps=[]

env = GridWorld()
e=[]
for episode in range(num_episodes):
    epsilon=epsilon*0.995
    e.append(epsilon)
    state = env.reset()
    done = False
    timestep = 180
    temp_reward=0;
    while (timestep>0 and not done ):
        timestep -= 1
        
        if np.random.random() < epsilon:
            action = env.action_space.sample()  
        else:
            action = np.argmax(Q[state])  
       
        next_state, reward, done, _ = env.step(action)
        if done==True:
            1==1
        #check deubg--> >>
        Q[state][action] = (1 - alpha) * Q[state][action] + alpha * (reward + gamma * np.max(Q[next_state]))
        temp_reward=temp_reward+reward
       
        state = next_state

    #print("timestep", timestep)
    tsteps.append(temp_reward);
    #print("episode no", episode);
# Print Q-table
print(Q)
#print("TimeStep array",tsteps)
plt.plot(tsteps,color="red")
plt.show()
plt.plot(e)
plt.show()
with open('StocQL.pkl', 'wb') as f:  # open a text file
    pickle.dump(Q, f) 
1==1 #debugger mark

with open('rl.pkl', 'rb') as f:
    Q = pickle.load(f) # deserialize using load()
    print(Q) # print student names

#determinticDQL
import gymnasium as gym
from gymnasium import spaces
import matplotlib.pyplot as plt
import numpy as np
import time

import numpy as np




class GridWorld(gym.Env):
    def __init__(self):
        self.observation_space = spaces.Discrete(25)
        self.action_space = spaces.Discrete(4)
        self.current_pos = [0, 0]
        self.reward1_pos = [0, 3]
        self.reward2_pos = [2, 1]
        self.reward3_pos = [3, 4]
        self.reward4_pos = [4, 4]
        self.reward5_pos = [0, 2]
        self.mosnter_pos = [3, 0]

    def step(self, action):

        if action == 0:
            self.current_pos[1] = min(self.current_pos[1] + 1, 4)
        elif action == 1:
            self.current_pos[1] = max(self.current_pos[1] - 1, 0)
        elif action == 2:
            self.current_pos[0] = max(self.current_pos[0] - 1, 0)
        elif action == 3:
            self.current_pos[0] = min(self.current_pos[0] + 1,  4)



        done = self.current_pos == [4,4]
        agentman = np.zeros((5, 5));
        agentman[self.current_pos[0]][self.current_pos[1]] = 1;

        reward=-1

        if done:
            reward = reward + 13
            if (reward > 0):
                1 == 1
        else:
            if np.array_equal(self.current_pos, self.reward1_pos):
                self.reward1_pos=[5,5]
                reward += 4
                if (reward > 0):
                    1 == 1

            if np.array_equal(self.current_pos, self.reward2_pos):
                self.reward2_pos=[5,5]
                reward += 7
                if (reward > 0):
                    1 == 1

            if np.array_equal(self.current_pos, self.reward3_pos):
                self.reward3_pos=[5,5]
                reward += 6
                if (reward > 0):
                    1 == 1



            if np.array_equal(self.current_pos, self.reward5_pos):
                self.reward5_pos=[5,5]
                reward += 3
                if (reward > 0):
                    1 == 1

            if np.array_equal(self.current_pos, self.mosnter_pos):
                self.mosnter_pos=[5,5]
                reward = reward -3
                if (reward > 0):
                    1 == 1

        #print("->", reward, self.current_pos)
        return self.current_pos[0] * 5 + self.current_pos[1], reward, done, {}

    def reset(self):
        self.current_pos = [0, 0]
        self.reward1_pos = [0, 3]
        self.reward2_pos = [2, 1]
        self.reward3_pos = [3, 4]
        self.reward4_pos = [4, 4]
        self.reward5_pos = [0, 2]
        self.mosnter_pos = [3, 0]
        #print("reset called", self.current_pos[0] * 5 + self.current_pos[1])
        return self.current_pos[0] * 5 + self.current_pos[1]


# Hyperparameters for assignemnt
alpha = 0.2
gamma = 0.99
epsilon = 1
num_episodes = 2000

#tables
Q1 = np.zeros((25, 4))
Q2 = np.zeros((25, 4))
tsteps=[]
# The QL is runnin
env = GridWorld()
e=[]
for episode in range(num_episodes):
    epsilon=epsilon*0.995
    e.append(epsilon)
    state = env.reset()
    done = False
    timestep = 180
    temp_reward=0;
    while (timestep>0 and not done ):
        timestep -= 1
        
        if np.random.random() < epsilon:
            action = env.action_space.sample()  
        else:
            Q = Q1 + Q2
            action = np.argmax(Q[state]) 

        
        next_state, reward, done, _ = env.step(action)
        if done==True:
            1==1
  
        if np.random.random() < 0.5:
            Q1[state][action] = (1 - alpha) * Q1[state][action] + alpha * (reward + gamma * np.max(Q1[next_state]))
        else:
          
            Q2[state][action] = (1 - alpha) * Q2[state][action] + alpha * (reward + gamma * Q2[next_state][np.argmax(Q2[next_state])])


        temp_reward=temp_reward+reward
       
        state = next_state

    #print("timestep", timestep)
    tsteps.append(temp_reward);
    #print("episode no", episode);
Temp=tsteps
print(Q1+Q2)
with open('determinDoubleQL.pkl', 'wb') as f:  # open a text file
    pickle.dump(Q1+Q2, f) 
#print("TimeStep array",tsteps)
plt.plot(tsteps,color="red")
plt.show()
plt.plot(e)
plt.show()
1==1

plt.plot(e)
plt.show()

#StochasticDQL
import gymnasium as gym
from gymnasium import spaces
import matplotlib.pyplot as plt
import numpy as np
import time

import numpy as np


class GridWorld(gym.Env):
    def __init__(self):
        self.observation_space = spaces.Discrete(25)
        self.action_space = spaces.Discrete(4)
        self.current_pos = [0, 0]
        self.reward1_pos = [0, 3]
        self.reward2_pos = [2, 1]
        self.reward3_pos = [3, 4]
        self.reward4_pos = [4, 4]
        self.reward5_pos = [0, 2]
        self.mosnter_pos = [3, 0]

    def step(self, action):

        if action == 0:  
            if (np.random.random() < 0.15):
                pass
            else:
                self.current_pos[1] = min(self.current_pos[1] + 1, 4)
        if action == 1:
            if (np.random.random() < 0.15):
                pass
            else:
                self.current_pos[1] = max(self.current_pos[1] - 1, 0)
        if action == 2:
            if (np.random.random() < 0.15):
                pass
            else:
                self.current_pos[0] = max(self.current_pos[0] - 1, 0)
        if action == 3:
            if (np.random.random() < 0.15):
                pass
            else:
                self.current_pos[0] = min(self.current_pos[0] + 1, 4)



        done = self.current_pos == [4,4]
        agentman = np.zeros((5, 5));
        agentman[self.current_pos[0]][self.current_pos[1]] = 1;

        reward=-1

        if done:
            reward = reward + 13
            if (reward > 0):
                1 == 1
        else:
            if np.array_equal(self.current_pos, self.reward1_pos):
                self.reward1_pos=[5,5]
                reward += 4
                if (reward > 0):
                    1 == 1

            if np.array_equal(self.current_pos, self.reward2_pos):
                self.reward2_pos=[5,5]
                reward += 7
                if (reward > 0):
                    1 == 1

            if np.array_equal(self.current_pos, self.reward3_pos):
                self.reward3_pos=[5,5]
                reward += 6
                if (reward > 0):
                    1 == 1



            if np.array_equal(self.current_pos, self.reward5_pos):
                self.reward5_pos=[5,5]
                reward += 3
                if (reward > 0):
                    1 == 1

            if np.array_equal(self.current_pos, self.mosnter_pos):
                self.mosnter_pos=[5,5]
                reward = reward -3
                if (reward > 0):
                    1 == 1

        return self.current_pos[0] * 5 + self.current_pos[1], reward, done, {}

    def reset(self):
        self.current_pos = [0, 0]
        self.reward1_pos = [0, 3]
        self.reward2_pos = [2, 1]
        self.reward3_pos = [3, 4]
        self.reward4_pos = [4, 4]
        self.reward5_pos = [0, 2]
        self.mosnter_pos = [3, 0]
        return self.current_pos[0] * 5 + self.current_pos[1]


alpha = 0.2
gamma = 0.99
epsilon = 1
num_episodes = 2000

Q1 = np.zeros((25, 4))
Q2 = np.zeros((25, 4))
tsteps=[]
# ql started
env = GridWorld()
e=[]
for episode in range(num_episodes):
    epsilon=epsilon*0.995
    e.append(epsilon)
    state = env.reset()
    done = False
    timestep = 180
    temp_reward=0;
    while (timestep>0 and not done ):
        timestep -= 1
       
        if np.random.random() < epsilon:
            action = env.action_space.sample()  
        else:
            Q = Q1 + Q2
            action = np.argmax(Q[state])  

        
        next_state, reward, done, _ = env.step(action)
        if done==True:
            1==1
        
        if np.random.random() < 0.5:
            Q1[state][action] = (1 - alpha) * Q1[state][action] + alpha * (reward + gamma * np.max(Q1[next_state]))
        else:
            
            Q2[state][action] = (1 - alpha) * Q2[state][action] + alpha * (reward + gamma * Q2[next_state][np.argmax(Q2[next_state])])


        temp_reward=temp_reward+reward
       
        state = next_state

    
    tsteps.append(temp_reward);
    

print(Q1+Q2)
with open('StocDQL.pkl', 'wb') as f:  # open a text file
    pickle.dump(Q, f) 

plt.plot(tsteps,color="red")
plt.show()
plt.plot(e)
plt.show()
1==1

# comparing determinsitcs
import gymnasium as gym
from gymnasium import spaces
import matplotlib.pyplot as plt
import numpy as np
import time

import numpy as np



class GridWorld(gym.Env):
    def __init__(self):
        self.observation_space = spaces.Discrete(25)
        self.action_space = spaces.Discrete(4)
        self.current_pos = [0, 0]
        self.reward1_pos = [0, 3]
        self.reward2_pos = [2, 1]
        self.reward3_pos = [3, 4]
        self.reward4_pos = [4, 4]
        self.reward5_pos = [0, 2]
        self.mosnter_pos = [3, 0]

    def step(self, action):

        if action == 0:
            self.current_pos[1] = min(self.current_pos[1] + 1, 4)
        elif action == 1:
            self.current_pos[1] = max(self.current_pos[1] - 1, 0)
        elif action == 2:
            self.current_pos[0] = max(self.current_pos[0] - 1, 0)
        elif action == 3:
            self.current_pos[0] = min(self.current_pos[0] + 1,  4)



        done = self.current_pos == [4,4]
        agentman = np.zeros((5, 5));
        agentman[self.current_pos[0]][self.current_pos[1]] = 1;

        reward=-1

        if done:
            reward = reward + 13
            if (reward > 0):
                1 == 1
        else:
            if np.array_equal(self.current_pos, self.reward1_pos):
                self.reward1_pos=[5,5]
                reward += 4
                if (reward > 0):
                    1 == 1

            if np.array_equal(self.current_pos, self.reward2_pos):
                self.reward2_pos=[5,5]
                reward += 7
                if (reward > 0):
                    1 == 1

            if np.array_equal(self.current_pos, self.reward3_pos):
                self.reward3_pos=[5,5]
                reward += 6
                if (reward > 0):
                    1 == 1



            if np.array_equal(self.current_pos, self.reward5_pos):
                self.reward5_pos=[5,5]
                reward += 3
                if (reward > 0):
                    1 == 1

            if np.array_equal(self.current_pos, self.mosnter_pos):
                self.mosnter_pos=[5,5]
                reward = reward -3
                if (reward > 0):
                    1 == 1

        #print("->", reward, self.current_pos)
        return self.current_pos[0] * 5 + self.current_pos[1], reward, done, {}

    def reset(self):
        self.current_pos = [0, 0]
        self.reward1_pos = [0, 3]
        self.reward2_pos = [2, 1]
        self.reward3_pos = [3, 4]
        self.reward4_pos = [4, 4]
        self.reward5_pos = [0, 2]
        self.mosnter_pos = [3, 0]
        #print("reset called", self.current_pos[0] * 5 + self.current_pos[1])
        return self.current_pos[0] * 5 + self.current_pos[1]



alpha = 0.2
gamma = 0.99
epsilon = 1
num_episodes = 2000


Q = np.zeros((25, 4))
tsteps=[]

env = GridWorld()
e=[]
for episode in range(num_episodes):
    epsilon=epsilon*0.995
    e.append(epsilon)
    state = env.reset()
    done = False
    timestep = 180
    temp_reward=0;
    while (timestep>0 and not done ):
        timestep -= 1
        if np.random.random() < epsilon:
            action = env.action_space.sample() 
        else:
            action = np.argmax(Q[state]) 

        
        next_state, reward, done, _ = env.step(action)
        if done==True:
            1==1
        
        Q[state][action] = (1 - alpha) * Q[state][action] + alpha * (reward + gamma * np.max(Q[next_state]))
        temp_reward=temp_reward+reward
        state = next_state

    #print("timestep", timestep)
    tsteps.append(temp_reward);
    #print("episode no", episode);

print(Q)
#print("TimeStep array",tsteps)

with open('rl.pkl', 'wb') as f:  # open a text file
    pickle.dump(Q, f) 
1==1

#Reward Fuction
import gymnasium as gym
from gymnasium import spaces
import matplotlib.pyplot as plt
import numpy as np
import time

import numpy as np




class GridWorld(gym.Env):
    def __init__(self):
        self.observation_space = spaces.Discrete(25)
        self.action_space = spaces.Discrete(4)
        self.current_pos = [0, 0]
        self.reward1_pos = [0, 3]
        self.reward2_pos = [2, 1]
        self.reward3_pos = [3, 4]
        self.reward4_pos = [4, 4]
        self.reward5_pos = [0, 2]
        self.mosnter_pos = [3, 0]

    def step(self, action):

        if action == 0:  # move right
            if (np.random.random() < 0.15):
                pass
            else:
                self.current_pos[1] = min(self.current_pos[1] + 1, 4)
        if action == 1:
            if (np.random.random() < 0.15):
                pass
            else:
                self.current_pos[1] = max(self.current_pos[1] - 1, 0)
        if action == 2:
            if (np.random.random() < 0.15):
                pass
            else:
                self.current_pos[0] = max(self.current_pos[0] - 1, 0)
        if action == 3:
            if (np.random.random() < 0.15):
                pass
            else:
                self.current_pos[0] = min(self.current_pos[0] + 1,  4)



        done = self.current_pos == [4,4]
        agentman = np.zeros((5, 5));
        agentman[self.current_pos[0]][self.current_pos[1]] = 1;

        reward=-1

        if done:
            reward = reward + 13
            if (reward > 0):
                1 == 1
        else:
            if np.array_equal(self.current_pos, self.reward1_pos):
                self.reward1_pos=[5,5]
                reward += 20
                if (reward > 0):
                    1 == 1

            if np.array_equal(self.current_pos, self.reward2_pos):
                self.reward2_pos=[5,5]
                reward += 7
                if (reward > 0):
                    1 == 1

            if np.array_equal(self.current_pos, self.reward3_pos):
                self.reward3_pos=[5,5]
                reward += 6
                if (reward > 0):
                    1 == 1



            if np.array_equal(self.current_pos, self.reward5_pos):
                self.reward5_pos=[5,5]
                reward += 3
                if (reward > 0):
                    1 == 1

            if np.array_equal(self.current_pos, self.mosnter_pos):
                self.mosnter_pos=[5,5]
                reward = reward -3
                if (reward > 0):
                    1 == 1

        #print("->", reward, self.current_pos)
        return self.current_pos[0] * 5 + self.current_pos[1], reward, done, {}

    def reset(self):
        self.current_pos = [0, 0]
        self.reward1_pos = [0, 3]
        self.reward2_pos = [2, 1]
        self.reward3_pos = [3, 4]
        self.reward4_pos = [4, 4]
        self.reward5_pos = [0, 2]
        self.mosnter_pos = [3, 0]
        #print("reset called", self.current_pos[0] * 5 + self.current_pos[1])
        return self.current_pos[0] * 5 + self.current_pos[1]


# Define hyperparameters
alpha = 0.2
gamma = 0.99
epsilon = 1
num_episodes = 500


Q = np.zeros((25, 4))
tsteps=[]

env = GridWorld()
e=[]
for episode in range(num_episodes):
    epsilon=epsilon*0.995
    e.append(epsilon)
    state = env.reset()
    done = False
    timestep = 180
    temp_reward=0;
    while (timestep>0 and not done ):
        timestep -= 1
        # Select action
        if np.random.random() < epsilon:
            action = env.action_space.sample()  # explore
        else:
            action = np.argmax(Q[state])  # exploit

        # Take action and observe next state and reward
        next_state, reward, done, _ = env.step(action)
        if done==True:
            1==1
        # Update Q-table
        Q[state][action] = (1 - alpha) * Q[state][action] + alpha * (reward + gamma * np.max(Q[next_state]))
        temp_reward=temp_reward+reward
        # Update state
        state = next_state

    #print("timestep", timestep)
    tsteps.append(temp_reward);
    #print("episode no", episode);
# Print Q-table
print(Q)
#print("TimeStep array",tsteps)
plt.plot(tsteps,color="red")
plt.show()
plt.plot(e)
plt.show()
with open('rl.pkl', 'wb') as f:  # open a text file
    pickle.dump(Q, f) 
1==1 #debugger mark

# grid world DETERMINISTIC environment with Q-Learning Algorithm
#hyperparameter tuning
import gymnasium as gym
from gymnasium import spaces
import matplotlib.pyplot as plt
import numpy as np
import time

import numpy as np



class GridWorld(gym.Env):
    def __init__(self):
        self.observation_space = spaces.Discrete(25)
        self.action_space = spaces.Discrete(4)
        self.current_pos = [0, 0]
        self.reward1_pos = [0, 3]
        self.reward2_pos = [2, 1]
        self.reward3_pos = [3, 4]
        self.reward4_pos = [4, 4]
        self.reward5_pos = [0, 2]
        self.mosnter_pos = [3, 0]

    def step(self, action):

        if action == 0:
            self.current_pos[1] = min(self.current_pos[1] + 1, 4)
        elif action == 1:
            self.current_pos[1] = max(self.current_pos[1] - 1, 0)
        elif action == 2:
            self.current_pos[0] = max(self.current_pos[0] - 1, 0)
        elif action == 3:
            self.current_pos[0] = min(self.current_pos[0] + 1,  4)



        done = self.current_pos == [4,4]
        agentman = np.zeros((5, 5));
        agentman[self.current_pos[0]][self.current_pos[1]] = 1;

        reward=-1

        if done:
            reward = reward + 13
            if (reward > 0):
                1 == 1
        else:
            if np.array_equal(self.current_pos, self.reward1_pos):
                self.reward1_pos=[5,5]
                reward += 4
                if (reward > 0):
                    1 == 1

            if np.array_equal(self.current_pos, self.reward2_pos):
                self.reward2_pos=[5,5]
                reward += 7
                if (reward > 0):
                    1 == 1

            if np.array_equal(self.current_pos, self.reward3_pos):
                self.reward3_pos=[5,5]
                reward += 6
                if (reward > 0):
                    1 == 1



            if np.array_equal(self.current_pos, self.reward5_pos):
                self.reward5_pos=[5,5]
                reward += 3
                if (reward > 0):
                    1 == 1

            if np.array_equal(self.current_pos, self.mosnter_pos):
                self.mosnter_pos=[5,5]
                reward = reward -3
                if (reward > 0):
                    1 == 1

        #print("->", reward, self.current_pos)
        return self.current_pos[0] * 5 + self.current_pos[1], reward, done, {}

    def reset(self):
        self.current_pos = [0, 0]
        self.reward1_pos = [0, 3]
        self.reward2_pos = [2, 1]
        self.reward3_pos = [3, 4]
        self.reward4_pos = [4, 4]
        self.reward5_pos = [0, 2]
        self.mosnter_pos = [3, 0]
        #print("reset called", self.current_pos[0] * 5 + self.current_pos[1])
        return self.current_pos[0] * 5 + self.current_pos[1]



alpha = 0.1
gamma = 0.99
epsilon = 1
num_episodes = 1000


Q = np.zeros((25, 4))
tsteps=[]

env = GridWorld()
e=[]
for episode in range(num_episodes):
    epsilon=epsilon*0.995
    e.append(epsilon)
    state = env.reset()
    done = False
    timestep = 180
    temp_reward=0;
    while (timestep>0 and not done ):
        timestep -= 1
        if np.random.random() < epsilon:
            action = env.action_space.sample() 
        else:
            action = np.argmax(Q[state]) 

        
        next_state, reward, done, _ = env.step(action)
        if done==True:
            1==1
        
        Q[state][action] = (1 - alpha) * Q[state][action] + alpha * (reward + gamma * np.max(Q[next_state]))
        temp_reward=temp_reward+reward
        state = next_state

    #print("timestep", timestep)
    tsteps.append(temp_reward);
    #print("episode no", episode);

#print("TimeStep array",tsteps)

plt.plot(tsteps,color="red")
plt.show()
with open('rl.pkl', 'wb') as f:  # open a text file
    pickle.dump(Q, f) 
1==1

# grid world DETERMINISTIC environment with Q-Learning Algorithm
#hyperparameter tuning -- timestep
import gymnasium as gym
from gymnasium import spaces
import matplotlib.pyplot as plt
import numpy as np
import time

import numpy as np



class GridWorld(gym.Env):
    def __init__(self):
        self.observation_space = spaces.Discrete(25)
        self.action_space = spaces.Discrete(4)
        self.current_pos = [0, 0]
        self.reward1_pos = [0, 3]
        self.reward2_pos = [2, 1]
        self.reward3_pos = [3, 4]
        self.reward4_pos = [4, 4]
        self.reward5_pos = [0, 2]
        self.mosnter_pos = [3, 0]

    def step(self, action):

        if action == 0:
            self.current_pos[1] = min(self.current_pos[1] + 1, 4)
        elif action == 1:
            self.current_pos[1] = max(self.current_pos[1] - 1, 0)
        elif action == 2:
            self.current_pos[0] = max(self.current_pos[0] - 1, 0)
        elif action == 3:
            self.current_pos[0] = min(self.current_pos[0] + 1,  4)



        done = self.current_pos == [4,4]
        agentman = np.zeros((5, 5));
        agentman[self.current_pos[0]][self.current_pos[1]] = 1;

        reward=-1

        if done:
            reward = reward + 13
            if (reward > 0):
                1 == 1
        else:
            if np.array_equal(self.current_pos, self.reward1_pos):
                self.reward1_pos=[5,5]
                reward += 4
                if (reward > 0):
                    1 == 1

            if np.array_equal(self.current_pos, self.reward2_pos):
                self.reward2_pos=[5,5]
                reward += 7
                if (reward > 0):
                    1 == 1

            if np.array_equal(self.current_pos, self.reward3_pos):
                self.reward3_pos=[5,5]
                reward += 6
                if (reward > 0):
                    1 == 1



            if np.array_equal(self.current_pos, self.reward5_pos):
                self.reward5_pos=[5,5]
                reward += 3
                if (reward > 0):
                    1 == 1

            if np.array_equal(self.current_pos, self.mosnter_pos):
                self.mosnter_pos=[5,5]
                reward = reward -3
                if (reward > 0):
                    1 == 1

        #print("->", reward, self.current_pos)
        return self.current_pos[0] * 5 + self.current_pos[1], reward, done, {}

    def reset(self):
        self.current_pos = [0, 0]
        self.reward1_pos = [0, 3]
        self.reward2_pos = [2, 1]
        self.reward3_pos = [3, 4]
        self.reward4_pos = [4, 4]
        self.reward5_pos = [0, 2]
        self.mosnter_pos = [3, 0]
        #print("reset called", self.current_pos[0] * 5 + self.current_pos[1])
        return self.current_pos[0] * 5 + self.current_pos[1]



alpha = 0.1
gamma = 0.99
epsilon = 1
num_episodes = 1000


Q = np.zeros((25, 4))
tsteps=[]

env = GridWorld()
e=[]
for episode in range(num_episodes):
    epsilon=epsilon*0.995
    e.append(epsilon)
    state = env.reset()
    done = False
    timestep = 180
    temp_reward=0;
    while (timestep>0 and not done ):
        timestep -= 1
        if np.random.random() < epsilon:
            action = env.action_space.sample() 
        else:
            action = np.argmax(Q[state]) 

        
        next_state, reward, done, _ = env.step(action)
        if done==True:
            1==1
        
        Q[state][action] = (1 - alpha) * Q[state][action] + alpha * (reward + gamma * np.max(Q[next_state]))
        temp_reward=temp_reward+reward
        state = next_state

    #print("timestep", timestep)
    tsteps.append(temp_reward);
    #print("episode no", episode);

#print("TimeStep array",tsteps)

plt.plot(tsteps,color="red")
plt.show()
with open('rl.pkl', 'wb') as f:  # open a text file
    pickle.dump(Q, f) 
1==1

# grid world DETERMINISTIC environment with Q-Learning Algorithm
#hyperparameter tuning -- epsilon decay
import gymnasium as gym
from gymnasium import spaces
import matplotlib.pyplot as plt
import numpy as np
import time

import numpy as np



class GridWorld(gym.Env):
    def __init__(self):
        self.observation_space = spaces.Discrete(25)
        self.action_space = spaces.Discrete(4)
        self.current_pos = [0, 0]
        self.reward1_pos = [0, 3]
        self.reward2_pos = [2, 1]
        self.reward3_pos = [3, 4]
        self.reward4_pos = [4, 4]
        self.reward5_pos = [0, 2]
        self.mosnter_pos = [3, 0]

    def step(self, action):

        if action == 0:
            self.current_pos[1] = min(self.current_pos[1] + 1, 4)
        elif action == 1:
            self.current_pos[1] = max(self.current_pos[1] - 1, 0)
        elif action == 2:
            self.current_pos[0] = max(self.current_pos[0] - 1, 0)
        elif action == 3:
            self.current_pos[0] = min(self.current_pos[0] + 1,  4)



        done = self.current_pos == [4,4]
        agentman = np.zeros((5, 5));
        agentman[self.current_pos[0]][self.current_pos[1]] = 1;

        reward=-1

        if done:
            reward = reward + 13
            if (reward > 0):
                1 == 1
        else:
            if np.array_equal(self.current_pos, self.reward1_pos):
                self.reward1_pos=[5,5]
                reward += 4
                if (reward > 0):
                    1 == 1

            if np.array_equal(self.current_pos, self.reward2_pos):
                self.reward2_pos=[5,5]
                reward += 7
                if (reward > 0):
                    1 == 1

            if np.array_equal(self.current_pos, self.reward3_pos):
                self.reward3_pos=[5,5]
                reward += 6
                if (reward > 0):
                    1 == 1



            if np.array_equal(self.current_pos, self.reward5_pos):
                self.reward5_pos=[5,5]
                reward += 3
                if (reward > 0):
                    1 == 1

            if np.array_equal(self.current_pos, self.mosnter_pos):
                self.mosnter_pos=[5,5]
                reward = reward -3
                if (reward > 0):
                    1 == 1

        #print("->", reward, self.current_pos)
        return self.current_pos[0] * 5 + self.current_pos[1], reward, done, {}

    def reset(self):
        self.current_pos = [0, 0]
        self.reward1_pos = [0, 3]
        self.reward2_pos = [2, 1]
        self.reward3_pos = [3, 4]
        self.reward4_pos = [4, 4]
        self.reward5_pos = [0, 2]
        self.mosnter_pos = [3, 0]
        #print("reset called", self.current_pos[0] * 5 + self.current_pos[1])
        return self.current_pos[0] * 5 + self.current_pos[1]



alpha = 0.1
gamma = 0.99
epsilon = 1
num_episodes = 1000


Q = np.zeros((25, 4))
tsteps=[]

env = GridWorld()
e=[]
for episode in range(num_episodes):
    epsilon=epsilon*0.993
    e.append(epsilon)
    state = env.reset()
    done = False
    timestep = 180
    temp_reward=0;
    while (timestep>0 and not done ):
        timestep -= 1
        if np.random.random() < epsilon:
            action = env.action_space.sample() 
        else:
            action = np.argmax(Q[state]) 

        
        next_state, reward, done, _ = env.step(action)
        if done==True:
            1==1
        
        Q[state][action] = (1 - alpha) * Q[state][action] + alpha * (reward + gamma * np.max(Q[next_state]))
        temp_reward=temp_reward+reward
        state = next_state

    #print("timestep", timestep)
    tsteps.append(temp_reward);
    #print("episode no", episode);

#print("TimeStep array",tsteps)

plt.plot(tsteps,color="red")
plt.show()
plt.plot(e)
plt.show()

with open('rl.pkl', 'wb') as f:  # open a text file
    pickle.dump(Q, f) 
1==1

# Imports
import gymnasium
from gymnasium import spaces
import math
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Defining the Stock Trading Environment.
"""DON'T MAKE ANY CHANGES TO THE ENVIRONMENT."""


class StockTradingEnvironment(gymnasium.Env):
    """This class implements the Stock Trading environment."""

    def __init__(self, file_path, train=True, number_of_days_to_consider=10):
        """This method initializes the environment.

        :param file_path: - Path of the CSV file containing the historical stock data.
        :param train: - Boolean indicating whether the goal is to train or test the performance of the agent.
        :param number_of_days_to_consider = Integer representing the number of days the for which the agent
                considers the trend in stock price to make a decision."""

        self.file_path = file_path
        self.stock_data = pd.read_csv(self.file_path)
        self.train = train

        # Splitting the data into train and test datasets.
        self.training_stock_data = self.stock_data.iloc[:int(0.8 * len(self.stock_data))]
        self.testing_stock_data = self.stock_data.iloc[int(0.8 * len(self.stock_data)):].reset_index()

        self.observation_space = spaces.Discrete(4)
        self.action_space = spaces.Discrete(3)

        self.investment_capital = 100000  # This defines the investment capital that the agent starts with.
        self.number_of_shares = 0  # This defines number of shares currently held by the agent.
        self.stock_value = 0  # This defines the value of the stock currently held by the agent.
        self.book_value = 0  # This defines the total value for which the agent bought the shares.
        # This defines the agent's total account value.
        self.total_account_value = self.investment_capital + self.stock_value
        # List to store the total account value over training or evaluation.
        self.total_account_value_list = []
        # This defines the number of days for which the agent considers the data before taking an action.
        self.number_of_days_to_consider = number_of_days_to_consider
        # The maximum timesteps the agent will take before the episode ends.
        if self.train:
            self.max_timesteps = len(self.training_stock_data) - self.number_of_days_to_consider
        else:
            self.max_timesteps = len(self.testing_stock_data) - self.number_of_days_to_consider
        # Initializing the number of steps taken to 0.
        self.timestep = 0
        self.reset()

    def reset(self):
        """This method resets the environment and returns the observation.

        :returns observation: - Integer in the range of 0 to 3 representing the four possible observations that the
                                agent can receive. The observation depends upon whether the price increased on average
                                in the number of days the agent considers, and whether the agent already has the stock
                                or not.

                 info: - info: - A dictionary that can be used to provide additional implementation information."""

        self.investment_capital = 100000  # This defines the investment capital that the agent starts with.
        self.number_of_shares = 0  # This defines number of shares currently held by the agent.
        self.stock_value = 0  # This defines the value of the stock currently held by the agent.
        self.book_value = 0  # This defines the total value for which the agent bought the shares.
        # This defines the agent's total account value.
        self.total_account_value = self.investment_capital + self.stock_value
        # List to store the total account value over training or evaluation.
        self.total_account_value_list = []
        # Initializing the number of steps taken to 0.
        self.timestep = 0

        # Getting the observation vector.
        if self.train:
            # If the task is to train the agent the maximum timesteps will be equal to the number of days considered
            # subtracted from the  length of the training stock data.
            self.max_timesteps = len(self.training_stock_data) - self.number_of_days_to_consider

            # Calculating whether the price increased or decreased/remained the same on the majority of days the agent
            # considers.
            price_increase_list = []
            for i in range(self.number_of_days_to_consider):
                if self.training_stock_data['Close'][self.timestep + 1 + i] \
                        - self.training_stock_data['Close'][self.timestep + i] > 0:
                    price_increase_list.append(1)
                else:
                    price_increase_list.append(0)

            if (np.sum(price_increase_list) / self.number_of_days_to_consider) >= 0.5:
                price_increase = True
            else:
                price_increase = False

            stock_held = False

            # Observation vector that will be passed to the agent.
            observation = [price_increase, stock_held]

        else:
            # If the task is to evaluate the trained agent's performance the maximum timesteps will be equal to the
            # number of days considered subtracted from the  length of the testing stock data.
            self.max_timesteps = len(self.testing_stock_data) - self.number_of_days_to_consider

            # Calculating whether the price increased or decreased/remained the same on the majority of days the agent
            # considers.
            price_increase_list = []
            for i in range(self.number_of_days_to_consider):
                if self.testing_stock_data['Close'][self.timestep + 1 + i] \
                        - self.testing_stock_data['Close'][self.timestep + i] > 0:
                    price_increase_list.append(1)
                else:
                    price_increase_list.append(0)

            if (np.sum(price_increase_list) / self.number_of_days_to_consider) >= 0.5:
                price_increase = True
            else:
                price_increase = False

            stock_held = False

            # Observation vector.
            observation = [price_increase, stock_held]

        if np.array_equal(observation, [True, False]):
            observation = 0
        if np.array_equal(observation, [True, True]):
            observation = 1
        if np.array_equal(observation, [False, False]):
            observation = 2
        if np.array_equal(observation, [False, True]):
            observation = 3

        info = None

        return observation, info

    def step(self, action):
        """This method implements what happens when the agent takes the action to Buy/Sell/Hold.

        :param action: - Integer in the range 0 to 2 inclusive.

        :returns observation: - Integer in the range of 0 to 3 representing the four possible observations that the
                                agent can receive. The observation depends upon whether the price increased on average
                                in the number of days the agent considers, and whether the agent already has the stock
                                or not.
                 reward: - Integer/Float value that's used to measure the performance of the agent.
                 terminated: - Boolean describing whether the episode has terminated.
                 truncated: - Boolean describing whether a truncation condition outside the scope of the MDP is satisfied.
                 info: - A dictionary that can be used to provide additional implementation information."""

        # We give the agent a penalty for taking actions such as buying a stock when the agent doesn't have the
        # investment capital and selling a stock when the agent doesn't have any shares.
        penalty = 0

        if self.train:
            if action == 0:  # Buy
                if self.number_of_shares > 0:
                    penalty = -10
                # Determining the number of shares the agent can buy.
                number_of_shares_to_buy = math.floor(self.investment_capital / self.training_stock_data[
                    'Open'][self.timestep + self.number_of_days_to_consider])
                # Adding to the number of shares the agent has.
                self.number_of_shares += number_of_shares_to_buy

                # Computing the stock value, book value, investment capital and reward.
                if number_of_shares_to_buy > 0:
                    self.stock_value +=\
                        self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \
                        * number_of_shares_to_buy
                    self.book_value += \
                        self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider]\
                        * number_of_shares_to_buy
                    self.investment_capital -= \
                        self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \
                        * number_of_shares_to_buy

                    reward = 1 + penalty

                else:
                    # Computing the stock value and reward.
                    self.stock_value = \
                        self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \
                        * self.number_of_shares
                    reward = -10

            if action == 1:  # Sell
                # Computing the investment capital, sell value and reward.
                self.investment_capital += \
                    self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \
                    * self.number_of_shares
                sell_value = self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \
                             * self.number_of_shares

                if self.book_value > 0:
                    reward = (sell_value - self.book_value) / self.book_value * 100
                else:
                    reward = -10

                self.number_of_shares = 0
                self.stock_value = 0
                self.book_value = 0

            if action == 2:  # Hold
                # Computing the stock value and reward.
                self.stock_value = self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \
                                   * self.number_of_shares

                if self.book_value > 0:
                    reward = (self.stock_value - self.book_value) / self.book_value * 100
                else:
                    reward = -1

        else:
            if action == 0:  # Buy
                if self.number_of_shares > 0:
                    penalty = -10
                # Determining the number of shares the agent can buy.
                number_of_shares_to_buy = math.floor(self.investment_capital / self.testing_stock_data[
                    'Open'][self.timestep + self.number_of_days_to_consider])
                # Adding to the number of shares the agent has.
                self.number_of_shares += number_of_shares_to_buy

                # Computing the stock value, book value, investment capital and reward.
                if number_of_shares_to_buy > 0:
                    self.stock_value += \
                        self.testing_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \
                        * number_of_shares_to_buy
                    self.book_value += \
                        self.testing_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \
                        * number_of_shares_to_buy
                    self.investment_capital -= \
                        self.testing_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \
                        * number_of_shares_to_buy

                    reward = 1 + penalty

                else:
                    # Computing the stock value and reward.
                    self.stock_value = self.training_stock_data['Open'][
                                           self.timestep + self.number_of_days_to_consider] * self.number_of_shares
                    reward = -10

            if action == 1:  # Sell
                # Computing the investment capital, sell value and reward.
                self.investment_capital += \
                    self.testing_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \
                    * self.number_of_shares
                sell_value = self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \
                             * self.number_of_shares

                if self.book_value > 0:
                    reward = (sell_value - self.book_value) / self.book_value * 100
                else:
                    reward = -10

                self.number_of_shares = 0
                self.stock_value = 0
                self.book_value = 0

            if action == 2:  # Hold
                # Computing the stock value and reward.
                self.stock_value = self.testing_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \
                                   * self.number_of_shares

                if self.book_value > 0:
                    reward = (self.stock_value - self.book_value) / self.book_value * 100
                else:
                    reward = -1

        # Determining if the agent currently has shares of the stock or not.
        if self.number_of_shares > 0:
            stock_held = True
        else:
            stock_held = False

        # Getting the observation vector.
        if self.train:
            # If the task is to train the agent the maximum timesteps will be equal to the number of days considered
            # subtracted from the  length of the training stock data.
            self.max_timesteps = len(self.training_stock_data) - self.number_of_days_to_consider

            # Calculating whether the price increased or decreased/remained the same on the majority of days the agent
            # considers.
            price_increase_list = []
            for i in range(self.number_of_days_to_consider):
                if self.training_stock_data['Close'][self.timestep + 1 + i] \
                        - self.training_stock_data['Close'][self.timestep + i] > 0:
                    price_increase_list.append(1)
                else:
                    price_increase_list.append(0)

            if (np.sum(price_increase_list) / self.number_of_days_to_consider) >= 0.5:
                price_increase = True
            else:
                price_increase = False

            # Observation vector.
            observation = [price_increase, stock_held]

        else:
            # If the task is to evaluate the trained agent's performance the maximum timesteps will be equal to the
            # number of days considered subtracted from the  length of the testing stock data.
            self.max_timesteps = len(self.testing_stock_data) - self.number_of_days_to_consider

            # Calculating whether the price increased or decreased/remained the same on the majority of days the agent
            # considers.
            price_increase_list = []
            for i in range(self.number_of_days_to_consider):
                if self.testing_stock_data['Close'][self.timestep + 1 + i] \
                        - self.testing_stock_data['Close'][self.timestep + i] > 0:
                    price_increase_list.append(1)
                else:
                    price_increase_list.append(0)

            if (np.sum(price_increase_list) / self.number_of_days_to_consider) >= 0.5:
                price_increase = True
            else:
                price_increase = False

            # Observation vector.
            observation = [price_increase, stock_held]

        self.timestep += 1  # Increasing the number of steps taken by the agent by 1.

        if np.array_equal(observation, [True, False]):
            observation = 0
        if np.array_equal(observation, [True, True]):
            observation = 1
        if np.array_equal(observation, [False, False]):
            observation = 2
        if np.array_equal(observation, [False, True]):
            observation = 3

        # Computing the total account value.
        self.total_account_value = self.investment_capital + self.stock_value
        # Appending the total account value of the list to plot the graph.
        self.total_account_value_list.append(self.total_account_value)

        # The episode terminates when the maximum timesteps have been reached.
        terminated = True if (self.timestep >= self.max_timesteps) \
            else False
        truncated = False
        info = {}

        return observation, reward, terminated, truncated, info

    def render(self, mode='human'):
        """This method renders the agent's total account value over time.

        :param mode: 'human' renders to the current display or terminal and returns nothing."""

        plt.figure(figsize=(15, 10))
        plt.plot(self.total_account_value_list, color='lightseagreen', linewidth=7)
        plt.xlabel('Days', fontsize=32)
        plt.ylabel('Total Account Value', fontsize=32)
        plt.title('Total Account Value over Time', fontsize=38)
        plt.grid()
        plt.show()

        # NOTE: You can adjust the parameter 'number_of_days_to_consider'

stock_trading_environment = StockTradingEnvironment('./NVDA(1).csv', number_of_days_to_consider=10)

# Define hyperparameters
alpha = 0.125
gamma = 0.99
epsilon = 1
num_episodes = 2000


# Initialize Q-table
Q = [[0,0,0],[0,0,0],[0,0,0],[0,0,0]]
tsteps=[]
# Run Q-learning algorithm
env = StockTradingEnvironment('./NVDA(1).csv', number_of_days_to_consider=10)
e=[]
for episode in range(num_episodes):
    epsilon=epsilon*0.995
    e.append(epsilon)
    state,inf = env.reset()
    done = False
    timestep = 170
    temp_reward=0;
    while (timestep>0 and not done ):
        timestep -= 1
        # Select action
        if np.random.random() < epsilon:
            action = env.action_space.sample()  # explore
        else:
            action = np.argmax(Q[state])  # exploit

        # Take action and observe next state and reward
        next_state, reward, done, _ ,info = env.step(action)
        if done==True:
            1==1
        # Update Q-table
        Q[state][action] = (1 - alpha) * Q[state][action] + alpha * (reward + gamma * np.max(Q[next_state]))
        temp_reward=temp_reward+reward
        # Update state
        state = next_state

    #print("timestep", timestep)
    tsteps.append(temp_reward);
    #print("episode no", episode);
# Print Q-table
print(Q)
print("TimeStep array",tsteps)
plt.plot(e)
plt.show()
plt.plot(tsteps,color="red")
plt.show()
env.render()
